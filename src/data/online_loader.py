"""
Online Medical Dataset Access - Stream data directly from web sources
No local downloads required!
"""

import requests
import cv2
import numpy as np
from io import BytesIO
from PIL import Image
import pandas as pd
from typing import List, Dict, Generator, Tuple
import os
import tempfile
import zipfile
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OnlineMedicalDataLoader:
    """Load medical datasets directly from online sources without downloading."""
    
    def __init__(self):
        """Initialize online data loader."""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Define online medical datasets with reliable sources
        self.datasets = {
            'brain_tumor_sample': {
                'name': 'Brain Tumor Sample Dataset',
                'type': 'classification',
                'classes': ['glioma', 'meningioma', 'notumor', 'pituitary'],
                'sample_urls': [
                    # Using reliable sample medical images from open datasets
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Glioma.jpg/256px-Glioma.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Meningioma%2C_WHO_grade_I.jpg/256px-Meningioma%2C_WHO_grade_I.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Normal_brain_MRI.jpg/256px-Normal_brain_MRI.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Pituitary_adenoma.jpg/256px-Pituitary_adenoma.jpg',
                ],
                'labels': ['glioma', 'meningioma', 'notumor', 'pituitary']
            },
            'covid_xray_sample': {
                'name': 'COVID-19 X-ray Sample Dataset',
                'type': 'classification', 
                'classes': ['covid', 'normal', 'pneumonia'],
                'sample_urls': [
                    # Sample chest X-ray images from Wikipedia medical commons
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Chest_X-ray_in_influenza_and_Haemophilus_influenzae%2C_posteroanterior%2C_annotated.jpg/256px-Chest_X-ray_in_influenza_and_Haemophilus_influenzae%2C_posteroanterior%2C_annotated.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Chest_X-ray_PA_3-30-2020.jpg/256px-Chest_X-ray_PA_3-30-2020.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Pneumonia_x-ray.jpg/256px-Pneumonia_x-ray.jpg',
                ],
                'labels': ['pneumonia', 'normal', 'pneumonia']
            },
            'chest_xray_sample': {
                'name': 'Chest X-ray Sample Dataset',
                'type': 'classification',
                'classes': ['normal', 'pneumonia'],
                'sample_urls': [
                    # Normal and abnormal chest X-rays from medical commons
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Chest_X-ray_%28Posteroanterior%29.jpg/256px-Chest_X-ray_%28Posteroanterior%29.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Pneumonia_x-ray.jpg/256px-Pneumonia_x-ray.jpg',
                    'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Chest_X-ray_in_influenza_and_Haemophilus_influenzae%2C_posteroanterior%2C_annotated.jpg/256px-Chest_X-ray_in_influenza_and_Haemophilus_influenzae%2C_posteroanterior%2C_annotated.jpg',
                ],
                'labels': ['normal', 'pneumonia', 'pneumonia']
            },
            'medical_demo': {
                'name': 'Medical Demo Dataset (Synthetic)',
                'type': 'classification',
                'classes': ['sample1', 'sample2'],
                'generate_synthetic': True,
                'sample_count': 50
            }
        }
    
    def stream_dataset(self, dataset_name: str, max_samples: int = 50) -> Generator[Tuple[np.ndarray, str], None, None]:
        """
        Stream medical images directly from online sources.
        
        Args:
            dataset_name: Name of the dataset to stream
            max_samples: Maximum number of samples to stream
            
        Yields:
            Tuple of (image_array, label)
        """
        if dataset_name not in self.datasets:
            raise ValueError(f"Dataset {dataset_name} not available. Available: {list(self.datasets.keys())}")
        
        dataset_info = self.datasets[dataset_name]
        logger.info(f"üåê Streaming {dataset_info['name']} from online sources...")
        
        # Check if this is a synthetic dataset
        if dataset_info.get('generate_synthetic', False):
            yield from self._generate_synthetic_medical_data(dataset_info, max_samples)
            return
        
        samples_streamed = 0
        
        for url, label in zip(dataset_info['sample_urls'], dataset_info['labels']):
            if samples_streamed >= max_samples:
                break
                
            try:
                # Stream image from URL
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                # Convert to image
                image = Image.open(BytesIO(response.content))
                
                # Convert to RGB if needed
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # Convert to numpy array
                img_array = np.array(image)
                
                yield img_array, label
                samples_streamed += 1
                
                logger.info(f"‚úÖ Streamed image {samples_streamed}: {label} ({img_array.shape})")
                
            except Exception as e:
                logger.warning(f"‚ùå Failed to stream {url}: {e}")
                continue
        
        # If we didn't get enough samples from online sources, generate synthetic ones
        if samples_streamed < max_samples and samples_streamed < 5:
            logger.info(f"üé≠ Generating synthetic samples to reach target...")
            remaining = max_samples - samples_streamed
            synthetic_info = {
                'classes': dataset_info['classes'],
                'sample_count': remaining
            }
            yield from self._generate_synthetic_medical_data(synthetic_info, remaining)
    
    def _generate_synthetic_medical_data(self, dataset_info: Dict, count: int) -> Generator[Tuple[np.ndarray, str], None, None]:
        """Generate synthetic medical images for testing and development."""
        logger.info(f"üé≠ Generating {count} synthetic medical images...")
        
        classes = dataset_info['classes']
        
        for i in range(count):
            # Create synthetic medical-like image
            height, width = 224, 224
            
            # Choose random class
            label = classes[i % len(classes)]
            
            # Generate different patterns based on class
            if 'tumor' in label.lower() or 'cancer' in label.lower():
                # Create darker regions for tumors
                img = np.random.randint(80, 150, (height, width, 3), dtype=np.uint8)
                # Add tumor-like spots
                center_x, center_y = np.random.randint(50, width-50), np.random.randint(50, height-50)
                cv2.circle(img, (center_x, center_y), np.random.randint(20, 40), (60, 60, 60), -1)
                
            elif 'normal' in label.lower() or 'notumor' in label.lower():
                # Create more uniform, lighter patterns
                img = np.random.randint(120, 200, (height, width, 3), dtype=np.uint8)
                # Add some texture
                noise = np.random.normal(0, 10, (height, width, 3))
                img = np.clip(img + noise, 0, 255).astype(np.uint8)
                
            elif 'covid' in label.lower() or 'pneumonia' in label.lower():
                # Create lung-like patterns with inflammation
                img = np.random.randint(90, 170, (height, width, 3), dtype=np.uint8)
                # Add cloudy patterns
                for _ in range(5):
                    center_x, center_y = np.random.randint(30, width-30), np.random.randint(30, height-30)
                    cv2.ellipse(img, (center_x, center_y), (20, 15), 0, 0, 360, (70, 70, 70), -1)
                    
            else:
                # Default medical image pattern
                img = np.random.randint(100, 180, (height, width, 3), dtype=np.uint8)
                # Add some medical-like structure
                cv2.rectangle(img, (50, 50), (width-50, height-50), (140, 140, 140), 2)
            
            # Add some realistic medical image characteristics
            # Slight blur to simulate medical imaging
            img = cv2.GaussianBlur(img, (3, 3), 0.5)
            
            # Add subtle gradient (common in medical images)
            gradient = np.linspace(0.9, 1.1, width).reshape(1, -1, 1)
            img = (img * gradient).astype(np.uint8)
            
            yield img, label
            
            if (i + 1) % 10 == 0:
                logger.info(f"Generated {i + 1}/{count} synthetic images...")
    
    
    def analyze_online_data_quality(self, dataset_name: str, max_samples: int = 20) -> Dict:
        """
        Analyze data quality of online medical dataset without downloading.
        
        Args:
            dataset_name: Name of the dataset to analyze
            max_samples: Maximum samples to analyze
            
        Returns:
            Quality analysis results
        """
        logger.info(f"üîç Analyzing online data quality for {dataset_name}...")
        
        analysis = {
            'dataset_name': dataset_name,
            'samples_analyzed': 0,
            'successful_loads': 0,
            'failed_loads': 0,
            'image_sizes': [],
            'image_channels': [],
            'class_distribution': {},
            'file_sizes_mb': [],
            'issues': []
        }
        
        for img_array, label in self.stream_dataset(dataset_name, max_samples):
            try:
                analysis['samples_analyzed'] += 1
                analysis['successful_loads'] += 1
                
                # Record image properties
                analysis['image_sizes'].append(img_array.shape[:2])
                analysis['image_channels'].append(img_array.shape[2] if len(img_array.shape) > 2 else 1)
                
                # Estimate file size (rough)
                analysis['file_sizes_mb'].append(img_array.nbytes / (1024*1024))
                
                # Class distribution
                analysis['class_distribution'][label] = analysis['class_distribution'].get(label, 0) + 1
                
                # Quality checks
                if img_array.max() == img_array.min():
                    analysis['issues'].append(f"Constant pixel values in {label} image")
                
                if img_array.shape[0] < 50 or img_array.shape[1] < 50:
                    analysis['issues'].append(f"Very small image size: {img_array.shape}")
                    
            except Exception as e:
                analysis['failed_loads'] += 1
                analysis['issues'].append(f"Processing error: {e}")
        
        # Calculate statistics
        if analysis['image_sizes']:
            sizes = np.array(analysis['image_sizes'])
            analysis['size_stats'] = {
                'mean_height': float(np.mean(sizes[:, 0])),
                'mean_width': float(np.mean(sizes[:, 1])),
                'size_variance': float(np.var(sizes.flatten())),
                'unique_sizes': len(np.unique(sizes, axis=0))
            }
        
        # Generate report
        self._print_online_quality_report(analysis)
        
        return analysis
    
    def _print_online_quality_report(self, analysis: Dict):
        """Print quality analysis report for online data."""
        print(f"\nüåê ONLINE MEDICAL DATA QUALITY REPORT")
        print("=" * 50)
        print(f"üìä Dataset: {analysis['dataset_name']}")
        print(f"üìà Samples analyzed: {analysis['samples_analyzed']}")
        print(f"‚úÖ Successful loads: {analysis['successful_loads']}")
        print(f"‚ùå Failed loads: {analysis['failed_loads']}")
        
        if analysis['successful_loads'] > 0:
            success_rate = (analysis['successful_loads'] / analysis['samples_analyzed']) * 100
            print(f"üìä Success rate: {success_rate:.1f}%")
        
        # Image statistics
        if 'size_stats' in analysis:
            stats = analysis['size_stats']
            print(f"\nüìè IMAGE PROPERTIES:")
            print(f"   Average size: {stats['mean_height']:.0f} x {stats['mean_width']:.0f}")
            print(f"   Unique sizes: {stats['unique_sizes']}")
            print(f"   Size variance: {stats['size_variance']:.1f}")
            
            if stats['size_variance'] > 10000:
                print(f"   ‚ö†Ô∏è High size variance - consider resizing")
        
        # Class distribution
        if analysis['class_distribution']:
            print(f"\nüè∑Ô∏è CLASS DISTRIBUTION:")
            total = sum(analysis['class_distribution'].values())
            for class_name, count in analysis['class_distribution'].items():
                percentage = (count / total) * 100
                print(f"   {class_name}: {count} ({percentage:.1f}%)")
        
        # Issues
        if analysis['issues']:
            print(f"\n‚ö†Ô∏è ISSUES DETECTED:")
            for issue in set(analysis['issues'][:5]):  # Show unique issues
                print(f"   ‚Ä¢ {issue}")
        
        # Recommendations
        print(f"\nüöÄ RECOMMENDATIONS:")
        print(f"   ‚úÖ Data accessible online - no storage needed!")
        print(f"   üìè Standardize image sizes for training")
        print(f"   üéØ Apply data augmentation for better training")
        print(f"   üíæ Cache processed images for faster training")
    
    def create_online_data_pipeline(self, dataset_name: str, target_size: Tuple[int, int] = (224, 224)) -> Generator:
        """
        Create a data pipeline that streams, preprocesses, and yields training data.
        
        Args:
            dataset_name: Dataset to stream
            target_size: Target image size for training
            
        Yields:
            Preprocessed (image, label) pairs
        """
        logger.info(f"üîÑ Creating online data pipeline for {dataset_name}")
        
        for img_array, label in self.stream_dataset(dataset_name, max_samples=100):
            try:
                # Resize image
                img_resized = cv2.resize(img_array, (target_size[1], target_size[0]))
                
                # Normalize pixel values
                img_normalized = img_resized.astype(np.float32) / 255.0
                
                # Convert label to categorical if needed
                dataset_info = self.datasets[dataset_name]
                if label in dataset_info['classes']:
                    label_idx = dataset_info['classes'].index(label)
                else:
                    label_idx = 0  # Default
                
                yield img_normalized, label_idx
                
            except Exception as e:
                logger.warning(f"Pipeline error: {e}")
                continue
    
    def test_online_access(self) -> bool:
        """Test if online medical datasets are accessible."""
        print("üåê Testing online medical dataset access...")
        
        accessible_datasets = []
        
        for dataset_name in self.datasets.keys():
            try:
                # Try to load one sample
                sample_count = 0
                for img, label in self.stream_dataset(dataset_name, max_samples=1):
                    sample_count += 1
                    break
                
                if sample_count > 0:
                    accessible_datasets.append(dataset_name)
                    print(f"   ‚úÖ {dataset_name}: Accessible")
                else:
                    print(f"   ‚ùå {dataset_name}: No samples loaded")
                    
            except Exception as e:
                print(f"   ‚ùå {dataset_name}: Error - {e}")
        
        print(f"\nüìä Summary: {len(accessible_datasets)}/{len(self.datasets)} datasets accessible")
        
        if accessible_datasets:
            print(f"üöÄ Ready to use online datasets: {accessible_datasets}")
            return True
        else:
            print(f"‚ùå No online datasets accessible. Check internet connection.")
            return False


def main():
    """Main function to test online data access."""
    loader = OnlineMedicalDataLoader()
    
    # Test online access
    if loader.test_online_access():
        print(f"\nüîç Running quality analysis...")
        
        # Analyze available datasets
        for dataset_name in ['brain_tumor_sample']:
            try:
                loader.analyze_online_data_quality(dataset_name, max_samples=10)
            except Exception as e:
                print(f"‚ùå Analysis failed for {dataset_name}: {e}")
    
    return loader


if __name__ == "__main__":
    main()
